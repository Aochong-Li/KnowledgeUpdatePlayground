{
  "os": "Linux-5.4.0-200-generic-x86_64-with-glibc2.31",
  "python": "3.12.4",
  "startedAt": "2024-12-23T06:31:36.947573Z",
  "args": [
    "--model_name=/share/goyal/lio/knowledge_update/continued_pretraining/model/new_knowledge-newhandpicked_rephrased5news-lr5e-06-rt1-rr0.1-epochs5-bs16-wd0.01-warmup0.05-Llama3.18B/checkpoint-614",
    "--block_size=2048",
    "--per_device_train_batch_size=1",
    "--per_device_eval_batch_size=3",
    "--gradient_accumulation_steps=4",
    "--num_train_epochs=1",
    "--learning_rate=1e-06",
    "--repeat_time=1",
    "--replay_rate=0.1",
    "--subsample_ratio=0.02",
    "--overwrite_output_dir=True",
    "--task_name=instruct",
    "--split_name=ultrachat-train",
    "--logging_steps=1",
    "--run_name=scaling-subsample_ratio0.02-instruct-ultrachat-train-lr1e-06-rt1-rr0.1-epochs1-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint614",
    "--bf16=True",
    "--output_dir=/share/goyal/lio/knowledge_update/continued_pretraining/model/scaling-subsample_ratio0.02-instruct-ultrachat-train-lr1e-06-rt1-rr0.1-epochs1-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint614",
    "--weight_decay=0.01",
    "--warmup_ratio=0.05",
    "--evaluation_strategy=no",
    "--save_strategy=epoch",
    "--save_total_limit=10",
    "--lr_scheduler_type=cosine",
    "--log_level=info",
    "--fsdp=hybrid_shard auto_wrap",
    "--fsdp_config=scripts/config/fsdp_config.json"
  ],
  "program": "/home/al2644/research/codebase/knowledge_update/continued_pretraining/train.py",
  "codePath": "train.py",
  "email": "al2644@cornell.edu",
  "root": "/home/al2644/research/codebase/knowledge_update/continued_pretraining",
  "host": "goyal-compute-01.cs.cornell.edu",
  "username": "al2644",
  "executable": "/home/al2644/anaconda3/envs/nlp/bin/python",
  "codePathLocal": "train.py",
  "cpu_count": 64,
  "cpu_count_logical": 64,
  "gpu": "NVIDIA H100 NVL",
  "gpu_count": 2,
  "disk": {
    "/": {
      "total": "85857402880",
      "used": "64937644032"
    }
  },
  "memory": {
    "total": "1081893679104"
  },
  "cpu": {
    "count": 64,
    "countLogical": 64
  },
  "gpu_nvidia": [
    {
      "name": "NVIDIA H100 NVL",
      "memoryTotal": "100485038080",
      "cudaCores": 16896,
      "architecture": "Hopper"
    },
    {
      "name": "NVIDIA H100 NVL",
      "memoryTotal": "100485038080",
      "cudaCores": 16896,
      "architecture": "Hopper"
    }
  ],
  "slurm": {
    "cluster_name": "g2",
    "conf": "/usr/local/slurm/slurm-22.05.8/etc/slurm.conf",
    "cpus_on_node": "32",
    "get_user_env": "1",
    "gpus_on_node": "2",
    "gtids": "0",
    "job_account": "goyal",
    "job_cpus_per_node": "32",
    "job_gid": "1753415",
    "job_gpus": "0,1",
    "job_id": "6294815",
    "job_name": "instruct_cptmodel_ultrachat",
    "job_nodelist": "goyal-compute-01",
    "job_num_nodes": "1",
    "job_partition": "goyal",
    "job_qos": "normal",
    "job_uid": "1743463",
    "job_user": "al2644",
    "jobid": "6294815",
    "localid": "0",
    "mem_per_node": "180000",
    "nnodes": "1",
    "node_aliases": "(null)",
    "nodeid": "0",
    "nodelist": "goyal-compute-01",
    "nprocs": "32",
    "ntasks": "32",
    "prio_process": "0",
    "procid": "0",
    "submit_dir": "/home/al2644/research",
    "submit_host": "g2-login-05.coecis.cornell.edu",
    "task_pid": "3124465",
    "tasks_per_node": "32",
    "topology_addr": "goyal-compute-01",
    "topology_addr_pattern": "node",
    "working_cluster": "g2:slurmctld:6817:9728:109"
  },
  "cudaVersion": "12.2"
}