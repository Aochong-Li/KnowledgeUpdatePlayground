 33%|███████████████████████████████████████████████████▎                                                                                                      | 19/57 [00:41<01:19,  2.10s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19
{'loss': 2.9831, 'grad_norm': 1722.766845703125, 'learning_rate': 3.333333333333333e-07, 'epoch': 0.05}
{'loss': 3.0014, 'grad_norm': 1606.059814453125, 'learning_rate': 6.666666666666666e-07, 'epoch': 0.1}
{'loss': 3.2791, 'grad_norm': 15675.2265625, 'learning_rate': 1e-06, 'epoch': 0.16}
{'loss': 2.6711, 'grad_norm': 1614.6971435546875, 'learning_rate': 9.991540791356342e-07, 'epoch': 0.21}
{'loss': 2.3811, 'grad_norm': 744.374755859375, 'learning_rate': 9.966191788709714e-07, 'epoch': 0.26}
{'loss': 2.3387, 'grad_norm': 17.96304702758789, 'learning_rate': 9.92403876506104e-07, 'epoch': 0.31}
{'loss': 2.6592, 'grad_norm': 5502.92724609375, 'learning_rate': 9.865224352899118e-07, 'epoch': 0.36}
{'loss': 2.258, 'grad_norm': 28.326730728149414, 'learning_rate': 9.789947561577443e-07, 'epoch': 0.42}
{'loss': 2.2571, 'grad_norm': 31.798629760742188, 'learning_rate': 9.698463103929541e-07, 'epoch': 0.47}
{'loss': 2.1143, 'grad_norm': 302.8336181640625, 'learning_rate': 9.591080534401371e-07, 'epoch': 0.52}
{'loss': 2.1326, 'grad_norm': 38.459632873535156, 'learning_rate': 9.468163201617061e-07, 'epoch': 0.57}
{'loss': 2.3302, 'grad_norm': 1779.8026123046875, 'learning_rate': 9.330127018922193e-07, 'epoch': 0.62}
{'loss': 2.1116, 'grad_norm': 931.2531127929688, 'learning_rate': 9.177439057064682e-07, 'epoch': 0.68}
{'loss': 1.984, 'grad_norm': 18.23909568786621, 'learning_rate': 9.010615963775219e-07, 'epoch': 0.73}
{'loss': 2.2336, 'grad_norm': 1981.455810546875, 'learning_rate': 8.83022221559489e-07, 'epoch': 0.78}
{'loss': 2.177, 'grad_norm': 2022.0966796875, 'learning_rate': 8.636868207865243e-07, 'epoch': 0.83}
{'loss': 2.0306, 'grad_norm': 48.62158966064453, 'learning_rate': 8.431208189343669e-07, 'epoch': 0.88}
{'loss': 2.0481, 'grad_norm': 615.4531860351562, 'learning_rate': 8.213938048432696e-07, 'epoch': 0.94}
{'loss': 1.9874, 'grad_norm': 74.66802978515625, 'learning_rate': 7.985792958513931e-07, 'epoch': 0.99}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19/model.safetensors.index.json.
2024-12-31 17:56:57,180 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19/pytorch_model_fsdp.bin
2024-12-31 17:57:45,781 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19/pytorch_model_fsdp.bin
2024-12-31 17:58:13,521 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19/optimizer.bin
2024-12-31 17:59:55,549 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-19/optimizer.bin
 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 38/57 [06:24<00:42,  2.24s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38
{'loss': 2.0293, 'grad_norm': 77.9730224609375, 'learning_rate': 7.74754489035403e-07, 'epoch': 1.04}
{'loss': 1.9744, 'grad_norm': 45.786338806152344, 'learning_rate': 7.5e-07, 'epoch': 1.09}
{'loss': 1.9792, 'grad_norm': 69.49195098876953, 'learning_rate': 7.243995901002311e-07, 'epoch': 1.14}
{'loss': 1.976, 'grad_norm': 104.84150695800781, 'learning_rate': 6.980398830195784e-07, 'epoch': 1.19}
{'loss': 1.9244, 'grad_norm': 70.87773895263672, 'learning_rate': 6.710100716628344e-07, 'epoch': 1.25}
{'loss': 2.016, 'grad_norm': 111.1330337524414, 'learning_rate': 6.434016163555451e-07, 'epoch': 1.3}
{'loss': 1.928, 'grad_norm': 43.48683166503906, 'learning_rate': 6.153079353712201e-07, 'epoch': 1.35}
{'loss': 1.8386, 'grad_norm': 39.35013961791992, 'learning_rate': 5.868240888334652e-07, 'epoch': 1.4}
{'loss': 1.8484, 'grad_norm': 16.485776901245117, 'learning_rate': 5.580464570626151e-07, 'epoch': 1.45}
{'loss': 1.8385, 'grad_norm': 50.79488754272461, 'learning_rate': 5.290724144552379e-07, 'epoch': 1.51}
{'loss': 1.8927, 'grad_norm': 77.29683685302734, 'learning_rate': 5e-07, 'epoch': 1.56}
{'loss': 1.8599, 'grad_norm': 38.97804260253906, 'learning_rate': 4.7092758554476206e-07, 'epoch': 1.61}
{'loss': 1.8608, 'grad_norm': 104.0470962524414, 'learning_rate': 4.419535429373848e-07, 'epoch': 1.66}
{'loss': 1.8171, 'grad_norm': 31.281007766723633, 'learning_rate': 4.131759111665348e-07, 'epoch': 1.71}
{'loss': 1.8016, 'grad_norm': 15.597746849060059, 'learning_rate': 3.846920646287799e-07, 'epoch': 1.77}
{'loss': 1.8541, 'grad_norm': 113.92481231689453, 'learning_rate': 3.56598383644455e-07, 'epoch': 1.82}
{'loss': 1.8285, 'grad_norm': 24.514902114868164, 'learning_rate': 3.2898992833716563e-07, 'epoch': 1.87}
{'loss': 1.9243, 'grad_norm': 143.83355712890625, 'learning_rate': 3.0196011698042156e-07, 'epoch': 1.92}
{'loss': 1.812, 'grad_norm': 83.81746673583984, 'learning_rate': 2.756004098997689e-07, 'epoch': 1.97}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38/model.safetensors.index.json.
2024-12-31 18:02:45,483 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38/pytorch_model_fsdp.bin
2024-12-31 18:03:39,038 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38/pytorch_model_fsdp.bin
2024-12-31 18:04:10,532 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38/optimizer.bin
2024-12-31 18:05:59,488 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-38/optimizer.bin
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [12:28<00:00,  2.27s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57
{'loss': 1.8109, 'grad_norm': 28.867576599121094, 'learning_rate': 2.500000000000001e-07, 'epoch': 2.03}
{'loss': 1.8522, 'grad_norm': 115.4535140991211, 'learning_rate': 2.25245510964597e-07, 'epoch': 2.08}
{'loss': 1.7989, 'grad_norm': 10.883068084716797, 'learning_rate': 2.01420704148607e-07, 'epoch': 2.13}
{'loss': 1.7692, 'grad_norm': 23.049468994140625, 'learning_rate': 1.7860619515673032e-07, 'epoch': 2.18}
{'loss': 1.803, 'grad_norm': 35.243080139160156, 'learning_rate': 1.5687918106563325e-07, 'epoch': 2.23}
{'loss': 1.8871, 'grad_norm': 107.4823226928711, 'learning_rate': 1.3631317921347562e-07, 'epoch': 2.29}
{'loss': 1.8145, 'grad_norm': 65.17030334472656, 'learning_rate': 1.1697777844051104e-07, 'epoch': 2.34}
{'loss': 1.8815, 'grad_norm': 95.14239501953125, 'learning_rate': 9.893840362247807e-08, 'epoch': 2.39}
{'loss': 1.7869, 'grad_norm': 53.41246032714844, 'learning_rate': 8.225609429353186e-08, 'epoch': 2.44}
{'loss': 1.7543, 'grad_norm': 33.26112365722656, 'learning_rate': 6.698729810778064e-08, 'epoch': 2.49}
{'loss': 1.7738, 'grad_norm': 16.82097816467285, 'learning_rate': 5.318367983829392e-08, 'epoch': 2.55}
{'loss': 1.7801, 'grad_norm': 40.44050979614258, 'learning_rate': 4.089194655986306e-08, 'epoch': 2.6}
{'loss': 1.7621, 'grad_norm': 22.848085403442383, 'learning_rate': 3.015368960704584e-08, 'epoch': 2.65}
{'loss': 1.7507, 'grad_norm': 12.990882873535156, 'learning_rate': 2.100524384225555e-08, 'epoch': 2.7}
{'loss': 1.7644, 'grad_norm': 76.40121459960938, 'learning_rate': 1.3477564710088096e-08, 'epoch': 2.75}
{'loss': 1.7725, 'grad_norm': 16.61579704284668, 'learning_rate': 7.59612349389599e-09, 'epoch': 2.81}
{'loss': 1.723, 'grad_norm': 16.454587936401367, 'learning_rate': 3.380821129028488e-09, 'epoch': 2.86}
{'loss': 1.8062, 'grad_norm': 49.32868576049805, 'learning_rate': 8.459208643659121e-10, 'epoch': 2.91}
{'loss': 1.7458, 'grad_norm': 11.252132415771484, 'learning_rate': 0.0, 'epoch': 2.96}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/model.safetensors.index.json.
2024-12-31 18:08:38,460 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/pytorch_model_fsdp.bin
2024-12-31 18:09:19,153 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/pytorch_model_fsdp.bin
2024-12-31 18:09:49,661 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/optimizer.bin
2024-12-31 18:11:13,119 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/optimizer.bin
Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/model.safetensors.index.json.
2024-12-31 18:13:13,878 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/pytorch_model_fsdp.bin
2024-12-31 18:14:00,493 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/pytorch_model_fsdp.bin
2024-12-31 18:14:31,536 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/optimizer.bin
2024-12-31 18:15:57,613 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/checkpoint-57/optimizer.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [21:47<00:00, 22.94s/it]
{'train_runtime': 1312.6937, 'train_samples_per_second': 0.352, 'train_steps_per_second': 0.043, 'train_loss': 2.0178796429383126, 'epoch': 2.96}
Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-06-rt1-rr0.0-epochs3-bs32-wd0.01-warmup0.05-new_knowledgenewhandpicked_rephrased5newslr5e06rt1rr0.1epochs5bs16wd0.01warmup0.05Llama3.18B_checkpoint1228/model.safetensors.index.json.
