 10%|█████████▎                                                                                   | 11/110 [00:08<01:02,  1.58it/s]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11
{'loss': 2.7941, 'grad_norm': inf, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.09}
{'loss': 2.868, 'grad_norm': inf, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.18}
{'loss': 2.7817, 'grad_norm': 1391.6890869140625, 'learning_rate': 5e-06, 'epoch': 0.27}
{'loss': 2.4334, 'grad_norm': 158.1413116455078, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.36}
{'loss': 2.2899, 'grad_norm': 32.523555755615234, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.45}
{'loss': 2.1429, 'grad_norm': 29.122488021850586, 'learning_rate': 1e-05, 'epoch': 0.55}
{'loss': 1.999, 'grad_norm': 14.010834693908691, 'learning_rate': 9.997718922447669e-06, 'epoch': 0.64}
{'loss': 1.9361, 'grad_norm': 22.504453659057617, 'learning_rate': 9.990877771116588e-06, 'epoch': 0.73}
{'loss': 1.919, 'grad_norm': 14.042083740234375, 'learning_rate': 9.979482788085455e-06, 'epoch': 0.82}
{'loss': 1.9287, 'grad_norm': 13.764570236206055, 'learning_rate': 9.96354437049027e-06, 'epoch': 0.91}
{'loss': 1.8443, 'grad_norm': 11.066959381103516, 'learning_rate': 9.943077061037672e-06, 'epoch': 1.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11/model.safetensors.index.json.
2024-12-18 21:35:25,704 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11/pytorch_model_fsdp.bin
2024-12-18 21:36:31,580 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11/pytorch_model_fsdp.bin
2024-12-18 21:36:59,765 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11/optimizer.bin
2024-12-18 21:38:44,727 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-11/optimizer.bin
 20%|██████████████████▌                                                                          | 22/110 [05:36<04:54,  3.35s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22
{'loss': 1.5588, 'grad_norm': 12.378870010375977, 'learning_rate': 9.91809953473572e-06, 'epoch': 1.09}
{'loss': 1.6119, 'grad_norm': 11.34260368347168, 'learning_rate': 9.888634581854235e-06, 'epoch': 1.18}
{'loss': 1.3648, 'grad_norm': 60.61701965332031, 'learning_rate': 9.854709087130261e-06, 'epoch': 1.27}
{'loss': 1.5359, 'grad_norm': 25.059890747070312, 'learning_rate': 9.816354005237583e-06, 'epoch': 1.36}
{'loss': 1.4781, 'grad_norm': 16.897706985473633, 'learning_rate': 9.77360433254273e-06, 'epoch': 1.45}
{'loss': 1.5267, 'grad_norm': 15.614727020263672, 'learning_rate': 9.726499075173201e-06, 'epoch': 1.55}
{'loss': 1.2689, 'grad_norm': 44.63582992553711, 'learning_rate': 9.675081213427076e-06, 'epoch': 1.64}
{'loss': 1.6564, 'grad_norm': 126.66617584228516, 'learning_rate': 9.619397662556434e-06, 'epoch': 1.73}
{'loss': 2.0287, 'grad_norm': 57.82643127441406, 'learning_rate': 9.55949922996045e-06, 'epoch': 1.82}
{'loss': 1.2279, 'grad_norm': 12.399885177612305, 'learning_rate': 9.49544056882713e-06, 'epoch': 1.91}
{'loss': 1.543, 'grad_norm': 34.66573715209961, 'learning_rate': 9.427280128266049e-06, 'epoch': 2.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22/model.safetensors.index.json.
2024-12-18 21:40:56,887 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22/pytorch_model_fsdp.bin
2024-12-18 21:41:54,201 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22/pytorch_model_fsdp.bin
2024-12-18 21:42:25,023 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22/optimizer.bin
2024-12-18 21:44:08,258 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-22/optimizer.bin
 30%|███████████████████████████▉                                                                 | 33/110 [11:00<04:18,  3.36s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33
{'loss': 1.395, 'grad_norm': 26.893352508544922, 'learning_rate': 9.355080099977579e-06, 'epoch': 2.09}
{'loss': 1.0176, 'grad_norm': 12.618510246276855, 'learning_rate': 9.278906361507238e-06, 'epoch': 2.18}
{'loss': 0.9213, 'grad_norm': 36.6007080078125, 'learning_rate': 9.198828416136991e-06, 'epoch': 2.27}
{'loss': 0.9999, 'grad_norm': 16.860563278198242, 'learning_rate': 9.114919329468283e-06, 'epoch': 2.36}
{'loss': 1.2438, 'grad_norm': 13.094352722167969, 'learning_rate': 9.02725566275473e-06, 'epoch': 2.45}
{'loss': 0.8077, 'grad_norm': 11.915663719177246, 'learning_rate': 8.935917403045251e-06, 'epoch': 2.55}
{'loss': 1.1465, 'grad_norm': 13.943708419799805, 'learning_rate': 8.840987890201404e-06, 'epoch': 2.64}
{'loss': 0.7458, 'grad_norm': 14.861653327941895, 'learning_rate': 8.742553740855507e-06, 'epoch': 2.73}
{'loss': 0.8432, 'grad_norm': 11.885848045349121, 'learning_rate': 8.640704769378943e-06, 'epoch': 2.82}
{'loss': 0.811, 'grad_norm': 10.899992942810059, 'learning_rate': 8.535533905932739e-06, 'epoch': 2.91}
{'loss': 1.0572, 'grad_norm': 14.007665634155273, 'learning_rate': 8.4271371116752e-06, 'epoch': 3.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33/model.safetensors.index.json.
2024-12-18 21:46:18,322 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33/pytorch_model_fsdp.bin
2024-12-18 21:47:06,008 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33/pytorch_model_fsdp.bin
2024-12-18 21:47:37,923 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33/optimizer.bin
2024-12-18 21:49:22,394 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-33/optimizer.bin
 40%|█████████████████████████████████████▏                                                       | 44/110 [16:14<03:36,  3.28s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44
{'loss': 0.5829, 'grad_norm': 9.923822402954102, 'learning_rate': 8.315613291203977e-06, 'epoch': 3.09}
{'loss': 0.6975, 'grad_norm': 9.853937149047852, 'learning_rate': 8.20106420231244e-06, 'epoch': 3.18}
{'loss': 0.5237, 'grad_norm': 9.08949089050293, 'learning_rate': 8.083594363142717e-06, 'epoch': 3.27}
{'loss': 0.4809, 'grad_norm': 9.043902397155762, 'learning_rate': 7.963310956820085e-06, 'epoch': 3.36}
{'loss': 0.4754, 'grad_norm': 8.765334129333496, 'learning_rate': 7.84032373365578e-06, 'epoch': 3.45}
{'loss': 0.46, 'grad_norm': 8.565650939941406, 'learning_rate': 7.714744911007395e-06, 'epoch': 3.55}
{'loss': 0.6591, 'grad_norm': 9.777554512023926, 'learning_rate': 7.586689070888284e-06, 'epoch': 3.64}
{'loss': 0.5158, 'grad_norm': 57.80552291870117, 'learning_rate': 7.4562730554193875e-06, 'epoch': 3.73}
{'loss': 0.4322, 'grad_norm': 14.284440994262695, 'learning_rate': 7.323615860218844e-06, 'epoch': 3.82}
{'loss': 0.5706, 'grad_norm': 12.42563247680664, 'learning_rate': 7.188838525826702e-06, 'epoch': 3.91}
{'loss': 0.4055, 'grad_norm': 7.564272880554199, 'learning_rate': 7.052064027263785e-06, 'epoch': 4.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44/model.safetensors.index.json.
2024-12-18 21:51:32,254 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44/pytorch_model_fsdp.bin
2024-12-18 21:52:22,223 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44/pytorch_model_fsdp.bin
2024-12-18 21:52:52,478 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44/optimizer.bin
2024-12-18 21:54:37,157 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-44/optimizer.bin
 50%|██████████████████████████████████████████████▌                                              | 55/110 [21:29<03:00,  3.28s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55
{'loss': 0.2961, 'grad_norm': 6.241878032684326, 'learning_rate': 6.913417161825449e-06, 'epoch': 4.09}
{'loss': 0.3181, 'grad_norm': 10.221624374389648, 'learning_rate': 6.773024435212678e-06, 'epoch': 4.18}
{'loss': 0.2878, 'grad_norm': 5.640525817871094, 'learning_rate': 6.631013946104348e-06, 'epoch': 4.27}
{'loss': 0.3178, 'grad_norm': 6.79179048538208, 'learning_rate': 6.487515269276015e-06, 'epoch': 4.36}
{'loss': 0.2802, 'grad_norm': 5.2079949378967285, 'learning_rate': 6.342659337371884e-06, 'epoch': 4.45}
{'loss': 0.3573, 'grad_norm': 5.891708850860596, 'learning_rate': 6.1965783214377895e-06, 'epoch': 4.55}
{'loss': 0.2564, 'grad_norm': 4.52830696105957, 'learning_rate': 6.049405510324237e-06, 'epoch': 4.64}
{'loss': 0.3199, 'grad_norm': 5.432436943054199, 'learning_rate': 5.90127518906953e-06, 'epoch': 4.73}
{'loss': 0.2697, 'grad_norm': 5.019834995269775, 'learning_rate': 5.752322516373916e-06, 'epoch': 4.82}
{'loss': 0.3522, 'grad_norm': 4.761518478393555, 'learning_rate': 5.6026834012766155e-06, 'epoch': 4.91}
{'loss': 0.2582, 'grad_norm': 5.506016731262207, 'learning_rate': 5.45249437914819e-06, 'epoch': 5.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55/model.safetensors.index.json.
2024-12-18 21:56:48,652 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55/pytorch_model_fsdp.bin
2024-12-18 21:57:37,626 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55/pytorch_model_fsdp.bin
2024-12-18 21:58:08,460 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55/optimizer.bin
2024-12-18 22:00:07,714 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-55/optimizer.bin
 60%|███████████████████████████████████████████████████████▊                                     | 66/110 [26:59<02:30,  3.42s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66
{'loss': 0.2062, 'grad_norm': 3.3051412105560303, 'learning_rate': 5.301892487111431e-06, 'epoch': 5.09}
{'loss': 0.2232, 'grad_norm': 3.8153507709503174, 'learning_rate': 5.151015139004445e-06, 'epoch': 5.18}
{'loss': 0.1963, 'grad_norm': 3.224090099334717, 'learning_rate': 5e-06, 'epoch': 5.27}
{'loss': 0.1953, 'grad_norm': 3.443416118621826, 'learning_rate': 4.848984860995557e-06, 'epoch': 5.36}
{'loss': 0.2027, 'grad_norm': 12.424239158630371, 'learning_rate': 4.69810751288857e-06, 'epoch': 5.45}
{'loss': 0.2087, 'grad_norm': 4.217896938323975, 'learning_rate': 4.547505620851812e-06, 'epoch': 5.55}
{'loss': 0.2227, 'grad_norm': 3.9745407104492188, 'learning_rate': 4.397316598723385e-06, 'epoch': 5.64}
{'loss': 0.2005, 'grad_norm': 3.764446258544922, 'learning_rate': 4.247677483626085e-06, 'epoch': 5.73}
{'loss': 0.2048, 'grad_norm': 10.15927505493164, 'learning_rate': 4.098724810930472e-06, 'epoch': 5.82}
{'loss': 0.1941, 'grad_norm': 122.7938232421875, 'learning_rate': 3.9505944896757635e-06, 'epoch': 5.91}
{'loss': 0.1962, 'grad_norm': 17.018171310424805, 'learning_rate': 3.803421678562213e-06, 'epoch': 6.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66/model.safetensors.index.json.
2024-12-18 22:02:21,272 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66/pytorch_model_fsdp.bin
2024-12-18 22:03:05,885 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66/pytorch_model_fsdp.bin
2024-12-18 22:03:36,764 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66/optimizer.bin
2024-12-18 22:05:14,306 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-66/optimizer.bin
 70%|█████████████████████████████████████████████████████████████████                            | 77/110 [32:06<01:46,  3.21s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77
{'loss': 0.1766, 'grad_norm': 4.342112064361572, 'learning_rate': 3.657340662628116e-06, 'epoch': 6.09}
{'loss': 0.178, 'grad_norm': 14.755254745483398, 'learning_rate': 3.5124847307239863e-06, 'epoch': 6.18}
{'loss': 0.1998, 'grad_norm': 5.797411918640137, 'learning_rate': 3.3689860538956547e-06, 'epoch': 6.27}
{'loss': 0.1815, 'grad_norm': 2.9071412086486816, 'learning_rate': 3.226975564787322e-06, 'epoch': 6.36}
{'loss': 0.1577, 'grad_norm': 3.981935739517212, 'learning_rate': 3.0865828381745515e-06, 'epoch': 6.45}
{'loss': 0.1715, 'grad_norm': 3.293198347091675, 'learning_rate': 2.947935972736217e-06, 'epoch': 6.55}
{'loss': 0.1591, 'grad_norm': 2.914551019668579, 'learning_rate': 2.8111614741732975e-06, 'epoch': 6.64}
{'loss': 0.1546, 'grad_norm': 3.738119125366211, 'learning_rate': 2.6763841397811576e-06, 'epoch': 6.73}
{'loss': 0.1628, 'grad_norm': 3.0503017902374268, 'learning_rate': 2.5437269445806146e-06, 'epoch': 6.82}
{'loss': 0.1575, 'grad_norm': 3.330509901046753, 'learning_rate': 2.4133109291117156e-06, 'epoch': 6.91}
{'loss': 0.1527, 'grad_norm': 3.537532091140747, 'learning_rate': 2.2852550889926067e-06, 'epoch': 7.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77/model.safetensors.index.json.
2024-12-18 22:07:27,398 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77/pytorch_model_fsdp.bin
2024-12-18 22:08:17,658 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77/pytorch_model_fsdp.bin
2024-12-18 22:08:49,142 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77/optimizer.bin
2024-12-18 22:10:21,286 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-77/optimizer.bin
 80%|██████████████████████████████████████████████████████████████████████████▍                  | 88/110 [37:13<01:10,  3.22s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88
{'loss': 0.1384, 'grad_norm': 2.6303670406341553, 'learning_rate': 2.159676266344222e-06, 'epoch': 7.09}
{'loss': 0.1394, 'grad_norm': 3.2397563457489014, 'learning_rate': 2.036689043179917e-06, 'epoch': 7.18}
{'loss': 0.1294, 'grad_norm': 3.5493903160095215, 'learning_rate': 1.9164056368572847e-06, 'epoch': 7.27}
{'loss': 0.1435, 'grad_norm': 3.42360258102417, 'learning_rate': 1.7989357976875603e-06, 'epoch': 7.36}
{'loss': 0.1296, 'grad_norm': 3.1505813598632812, 'learning_rate': 1.6843867087960252e-06, 'epoch': 7.45}
{'loss': 0.1405, 'grad_norm': 3.2591192722320557, 'learning_rate': 1.572862888324801e-06, 'epoch': 7.55}
{'loss': 0.1317, 'grad_norm': 5.933982849121094, 'learning_rate': 1.4644660940672628e-06, 'epoch': 7.64}
{'loss': 0.1346, 'grad_norm': 5.481313705444336, 'learning_rate': 1.3592952306210589e-06, 'epoch': 7.73}
{'loss': 0.1414, 'grad_norm': 3.3984713554382324, 'learning_rate': 1.257446259144494e-06, 'epoch': 7.82}
{'loss': 0.1212, 'grad_norm': 6.451898097991943, 'learning_rate': 1.159012109798598e-06, 'epoch': 7.91}
{'loss': 0.1311, 'grad_norm': 3.35750412940979, 'learning_rate': 1.0640825969547498e-06, 'epoch': 8.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88/model.safetensors.index.json.
2024-12-18 22:12:34,884 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88/pytorch_model_fsdp.bin
2024-12-18 22:13:25,953 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88/pytorch_model_fsdp.bin
2024-12-18 22:13:56,934 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88/optimizer.bin
2024-12-18 22:15:37,402 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-88/optimizer.bin
 90%|███████████████████████████████████████████████████████████████████████████████████▋         | 99/110 [42:29<00:36,  3.29s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99
{'loss': 0.1219, 'grad_norm': 3.150071144104004, 'learning_rate': 9.7274433724527e-07, 'epoch': 8.09}
{'loss': 0.1177, 'grad_norm': 3.065685749053955, 'learning_rate': 8.850806705317183e-07, 'epoch': 8.18}
{'loss': 0.1279, 'grad_norm': 3.3714725971221924, 'learning_rate': 8.011715838630107e-07, 'epoch': 8.27}
{'loss': 0.1118, 'grad_norm': 3.011134386062622, 'learning_rate': 7.210936384927631e-07, 'epoch': 8.36}
{'loss': 0.1216, 'grad_norm': 3.0696914196014404, 'learning_rate': 6.449199000224221e-07, 'epoch': 8.45}
{'loss': 0.1289, 'grad_norm': 3.254410982131958, 'learning_rate': 5.727198717339511e-07, 'epoch': 8.55}
{'loss': 0.1224, 'grad_norm': 3.540755271911621, 'learning_rate': 5.045594311728708e-07, 'epoch': 8.64}
{'loss': 0.123, 'grad_norm': 3.358947992324829, 'learning_rate': 4.405007700395497e-07, 'epoch': 8.73}
{'loss': 0.1183, 'grad_norm': 3.104921817779541, 'learning_rate': 3.8060233744356634e-07, 'epoch': 8.82}
{'loss': 0.1242, 'grad_norm': 3.496487855911255, 'learning_rate': 3.2491878657292643e-07, 'epoch': 8.91}
{'loss': 0.1216, 'grad_norm': 3.1669132709503174, 'learning_rate': 2.7350092482679836e-07, 'epoch': 9.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99/model.safetensors.index.json.
2024-12-18 22:17:50,983 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99/pytorch_model_fsdp.bin
2024-12-18 22:18:36,600 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99/pytorch_model_fsdp.bin
2024-12-18 22:19:07,534 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99/optimizer.bin
2024-12-18 22:20:49,874 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-99/optimizer.bin
100%|████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [47:42<00:00,  3.26s/it]Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110
{'loss': 0.1197, 'grad_norm': 3.1233487129211426, 'learning_rate': 2.2639566745727203e-07, 'epoch': 9.09}
{'loss': 0.1187, 'grad_norm': 3.5444931983947754, 'learning_rate': 1.8364599476241862e-07, 'epoch': 9.18}
{'loss': 0.1156, 'grad_norm': 3.75461745262146, 'learning_rate': 1.4529091286973994e-07, 'epoch': 9.27}
{'loss': 0.1157, 'grad_norm': 3.112539529800415, 'learning_rate': 1.1136541814576574e-07, 'epoch': 9.36}
{'loss': 0.127, 'grad_norm': 3.428273916244507, 'learning_rate': 8.190046526428241e-08, 'epoch': 9.45}
{'loss': 0.1168, 'grad_norm': 3.371093511581421, 'learning_rate': 5.6922938962329364e-08, 'epoch': 9.55}
{'loss': 0.121, 'grad_norm': 3.3020095825195312, 'learning_rate': 3.645562950973014e-08, 'epoch': 9.64}
{'loss': 0.1113, 'grad_norm': 3.1599416732788086, 'learning_rate': 2.0517211914545254e-08, 'epoch': 9.73}
{'loss': 0.1138, 'grad_norm': 3.4078128337860107, 'learning_rate': 9.12222888341252e-09, 'epoch': 9.82}
{'loss': 0.1181, 'grad_norm': 3.1791067123413086, 'learning_rate': 2.2810775523329775e-09, 'epoch': 9.91}
{'loss': 0.1166, 'grad_norm': 3.1986138820648193, 'learning_rate': 0.0, 'epoch': 10.0}
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/model.safetensors.index.json.
2024-12-18 22:23:03,696 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/pytorch_model_fsdp.bin
2024-12-18 22:23:47,719 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/pytorch_model_fsdp.bin
2024-12-18 22:24:18,834 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/optimizer.bin
2024-12-18 22:25:42,289 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/optimizer.bin
Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/model.safetensors.index.json.
2024-12-18 22:27:47,443 - INFO - Saving model to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/pytorch_model_fsdp.bin
2024-12-18 22:28:32,893 - INFO - Model saved to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/pytorch_model_fsdp.bin
2024-12-18 22:28:59,646 - INFO - Saving Optimizer state to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/optimizer.bin
2024-12-18 22:30:34,068 - INFO - Optimizer state saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/checkpoint-110/optimizer.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


100%|████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [57:19<00:00, 31.27s/it]
{'train_runtime': 3440.9061, 'train_samples_per_second': 0.064, 'train_steps_per_second': 0.032, 'train_loss': 0.6369212920015509, 'epoch': 10.0}
Saving model checkpoint to /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/config.json
Configuration saved in /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 7 checkpoint shards. You can find where each parameters has been saved in the index located at /share/goyal/lio/knowledge_update/continued_pretraining/model/instruct-explicitnews_sft-train-lr1e-05-rt1-rr0.1-epochs10-bs8-wd0.01-warmup0.05-new_knowledgenewhandpicked_augmenteddatalr5e06rt1rr0.1epochs1bs16wd0.01warmup0.05Llama3.18B/model.safetensors.index.json.
